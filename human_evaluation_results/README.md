# Texts Detoxification Outputs Human Evaluation Results
To consult the result of the human evaluation of your system output navigate to the folder with the corresponding team name and check tsv file with the necessary language

- toxic_sentence - original toxic sentence
- neutral_sentence - detoxified sentence generated by the system

Fluency is annotated first individually. The scores are 0 - unclear sentence, 0.5 - there are some significant mistkaes disturbing from understanding, 1 - minor mistakes or no mistkaes
- toxic_fluency - fluency of the toxic sentence
- neutral_fluency - fluency of the detoxified sentence
- fluency_score - overall fluency score (1 if neutral sentence fluency is greater or equal to the toxic sentence fluency and 0 otherwise)


Content similarity is annotated in a pairwise manner. The scores are 0 if the texts have different content, and 1 is similar.
- content_score - score of semantic similarity between original and generated sentence

Toxicity is annotated in a pairwise manner. The scores are 0 if the toxic sentence is voted as less toxic, 0.5 if a pair is annotated as "equally toxic", and 1 if the neutral text is annotated as less toxic.  The final value is averaged in this case.
- toxic_pairwise_score - score of toxicity (results from pairwise comparison of original and generated sentence)

To calculate the final metric for each language we
- average the scores of fluency_score, content_score, toxic_pairwise_score
- multiply averaged scores

The full texts of instructions per each task and per each language can be found in our repository in this [subfolder](https://github.com/textdetox/textdetox_clef_2024/tree/main/instructions/human_evaluation). Please take into account that the full description of the evaluation annotation setup will be available in the paper describing the results of the shared task.

